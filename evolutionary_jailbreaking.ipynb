{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,4,5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "\n",
    "ROUNDS = 10\n",
    "ATTACKS_PER_ROUND = 20\n",
    "SAMPLE_CAP = 1000\n",
    "OUTPUT_DIR = \"./content/experiment_jailbreak\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# toggles\n",
    "USE_PARAPHRASER = True \n",
    "\n",
    "# dataset\n",
    "DATASET_ID = \"JailbreakV-28K/JailBreakV-28k\"\n",
    "\n",
    "# random seeds\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "if DATASET_ID == \"JailbreakBench/JBB-Behaviors\":\n",
    "    ds_full = load_dataset(DATASET_ID, \"behaviors\")[\"harmful\"]\n",
    "    PROMPT_FIELD = \"Goal\"\n",
    "else:\n",
    "    ds_full = load_dataset(\"JailbreakV-28K/JailBreakV-28k\", \"JailBreakV_28K\")[\"JailBreakV_28K\"]\n",
    "    PROMPT_FIELD = \"jailbreak_query\"\n",
    "\n",
    "print(\"Dataset loaded. Number of examples:\", len(ds_full))\n",
    "\n",
    "first = ds_full[0]\n",
    "print(\"Available fields:\", list(first.keys()))\n",
    "print(\"Using prompt field:\", PROMPT_FIELD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_prompts = []\n",
    "seen = set()\n",
    "\n",
    "count = 0\n",
    "for item in ds_full:\n",
    "  if count >= SAMPLE_CAP:\n",
    "        break\n",
    "  raw = item.get(PROMPT_FIELD, None)\n",
    "  if raw and raw not in seen:\n",
    "    a_prompts.append(raw)\n",
    "    seen.add(raw)\n",
    "  count += 1\n",
    "print(\"Collected prompts:\", len(a_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_REAL_PARAPH = USE_PARAPHRASER\n",
    "hf_paraphraser = None\n",
    "PARAPH_MODEL = \"Vamsi/T5_Paraphrase_Paws\"\n",
    "\n",
    "if USE_REAL_PARAPH:\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "    try:\n",
    "        print(\"Loading paraphraser...\")\n",
    "        par_tok = AutoTokenizer.from_pretrained(PARAPH_MODEL)\n",
    "\n",
    "        par_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            PARAPH_MODEL,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            par_model.to(\"cuda\")\n",
    "\n",
    "        par_model.eval()\n",
    "        hf_paraphraser = (par_tok, par_model)\n",
    "        print(\"Paraphraser loaded.\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not load paraphraser, using fallback:\", e)\n",
    "        USE_REAL_PARAPH = False\n",
    "\n",
    "\n",
    "def paraphrase_text(prompt, n=4):\n",
    "    if USE_REAL_PARAPH and hf_paraphraser:\n",
    "        tok, model = hf_paraphraser\n",
    "\n",
    "        inputs = tok(\n",
    "            \"paraphrase: \" + prompt + \" </s>\",\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True\n",
    "        )\n",
    "        # put inputs on SAME device as the model\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        # generate paraphrases\n",
    "        gen = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=n,\n",
    "            top_p=0.95,\n",
    "            max_new_tokens=1000\n",
    "        )\n",
    "\n",
    "        out = [tok.decode(g, skip_special_tokens=True).strip() for g in gen]\n",
    "\n",
    "        seen = []\n",
    "        for o in out:\n",
    "            if o not in seen:\n",
    "                seen.append(o)\n",
    "\n",
    "        return seen[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "OFFLOAD_DIR = \"./content/mpt_offload\" \n",
    "os.makedirs(OFFLOAD_DIR, exist_ok=True)\n",
    "\n",
    "def load_llms():\n",
    "    MISTRAL_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "    mistral_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MISTRAL_MODEL_NAME,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MISTRAL_MODEL_NAME,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        offload_folder=OFFLOAD_DIR,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
    "    mistral_model.config.pad_token_id = mistral_model.config.eos_token_id\n",
    "    mistral_model.eval()\n",
    "\n",
    "    LLAMA_MODEL_ID = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "    llama_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=LLAMA_MODEL_ID,\n",
    "        model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"mistral\": {\n",
    "            \"tokenizer\": mistral_tokenizer,\n",
    "            \"model\": mistral_model,\n",
    "        },\n",
    "        \"llama\": llama_pipeline,\n",
    "    }\n",
    "\n",
    "models = load_llms()\n",
    "\n",
    "mistral_tok = models[\"mistral\"][\"tokenizer\"]\n",
    "mistral_model = models[\"mistral\"][\"model\"]\n",
    "llama = models[\"llama\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_mistral_llm(prompt, max_new_tokens=1000):\n",
    "    \"\"\"Generates a model reply from the LLM\"\"\"\n",
    "    inputs = mistral_tok(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        gen = mistral_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    output = mistral_tok.decode(gen[0], skip_special_tokens=True)\n",
    "    return output.replace(prompt, \"\", 1).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llama_llm(prompt_text: str) -> str:\n",
    "    out = llama(\n",
    "        prompt_text,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return out[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "SAFETY_MODEL_NAME = \"meta-llama/LlamaGuard-7B\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer_safe = AutoTokenizer.from_pretrained(SAFETY_MODEL_NAME)\n",
    "model_safe = AutoModelForCausalLM.from_pretrained(\n",
    "    SAFETY_MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "model_safe.eval()\n",
    "\n",
    "\n",
    "def classify_llamaguard(text, max_new_tokens=1000):\n",
    "    \"\"\"\n",
    "    Run LlamaGuard on a single text string and return its raw output text.\n",
    "    \"\"\"\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer_safe.apply_chat_template(\n",
    "        chat,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model_safe.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer_safe.eos_token_id,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    prompt_len = input_ids.shape[-1]\n",
    "    generated_ids = output_ids[0][prompt_len:]\n",
    "    generated_text = tokenizer_safe.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def is_unsafe_reply_llamaguard(text):\n",
    "\n",
    "    raw = classify_llamaguard(text)\n",
    "    return raw != 'safe'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "mistral_history = []\n",
    "mistral_global_attacks = []\n",
    "\n",
    "llama_history = []\n",
    "llama_global_attacks = []\n",
    "\n",
    "SEED_PROMPTS = 5\n",
    "SAVE_ONLY_SUCCESS = False \n",
    "\n",
    "current_bases = random.sample(a_prompts, k=min(SEED_PROMPTS, len(a_prompts)))\n",
    "\n",
    "print(f\"Starting iterative experiment:\")\n",
    "print(f\"ROUNDS={ROUNDS}, SEED_PROMPTS={len(current_bases)}, ATTACKS_PER_ROUND={ATTACKS_PER_ROUND}\")\n",
    "\n",
    "\n",
    "for r in range(1, ROUNDS + 1):\n",
    "    print(f\"\\n=== Round {r} ===\")\n",
    "    attacks = []\n",
    "\n",
    "    # Generate paraphrased variants from the current base prompts\n",
    "    for b in current_bases:\n",
    "        variants = paraphrase_text(b, n=3)\n",
    "        if variants:\n",
    "            attacks.extend(variants)\n",
    "\n",
    "    if ATTACKS_PER_ROUND is not None:\n",
    "        attacks = attacks[:ATTACKS_PER_ROUND]\n",
    "\n",
    "    if not attacks:\n",
    "        print(\"No attacks generated; stopping.\")\n",
    "        break\n",
    "\n",
    "    mistral_successes = []\n",
    "    llama_successes = []\n",
    "\n",
    "    for prompt_text in attacks:\n",
    "\n",
    "        try:\n",
    "            model_reply = query_mistral_llm(prompt_text)\n",
    "            flagged = is_unsafe_reply_llamaguard(model_reply)\n",
    "        except Exception as e:\n",
    "            print(\"Error querying Mistral:\", e)\n",
    "            model_reply = None\n",
    "            flagged = False\n",
    "\n",
    "        record = {\n",
    "            \"round\": r,\n",
    "            \"prompt\": prompt_text,\n",
    "            \"model_reply\": model_reply,\n",
    "            \"unsafe_marked\": flagged,\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        }\n",
    "\n",
    "        if SAVE_ONLY_SUCCESS:\n",
    "            if flagged:\n",
    "                mistral_global_attacks.append(record)\n",
    "        else:\n",
    "            mistral_global_attacks.append(record)\n",
    "\n",
    "        if flagged:\n",
    "            mistral_successes.append(prompt_text)\n",
    "\n",
    "        try:\n",
    "            llama_reply = query_llama_llm(prompt_text)\n",
    "            llama_flagged = is_unsafe_reply_llamaguard(llama_reply)\n",
    "        except Exception as e:\n",
    "            print(\"Error querying LLaMA:\", e)\n",
    "            llama_reply = None\n",
    "            llama_flagged = False\n",
    "\n",
    "        llama_record = {\n",
    "            \"round\": r,\n",
    "            \"prompt\": prompt_text,\n",
    "            \"model_reply\": llama_reply,\n",
    "            \"unsafe_marked\": llama_flagged,\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        }\n",
    "\n",
    "        if SAVE_ONLY_SUCCESS:\n",
    "            if llama_flagged:\n",
    "                llama_global_attacks.append(llama_record)\n",
    "        else:\n",
    "            llama_global_attacks.append(llama_record)\n",
    "\n",
    "        if llama_flagged:\n",
    "            llama_successes.append(prompt_text)\n",
    "\n",
    "    n_attacks = len(attacks)\n",
    "\n",
    "    mistral_asr = len(mistral_successes) / max(1, n_attacks)\n",
    "    llama_asr = len(llama_successes) / max(1, n_attacks)\n",
    "\n",
    "    print(f\"[Round {r}] Mistral: Attacks={n_attacks} Successes={len(mistral_successes)} ASR={mistral_asr:.4f}\")\n",
    "    print(f\"[Round {r}] LLaMA:   Attacks={n_attacks} Successes={len(llama_successes)} ASR={llama_asr:.4f}\")\n",
    "\n",
    "    mistral_history.append({\n",
    "        \"round\": r,\n",
    "        \"n_attacks\": n_attacks,\n",
    "        \"n_success\": len(mistral_successes),\n",
    "        \"ASR\": mistral_asr,\n",
    "    })\n",
    "    llama_history.append({\n",
    "        \"round\": r,\n",
    "        \"n_attacks\": n_attacks,\n",
    "        \"n_success\": len(llama_successes),\n",
    "        \"ASR\": llama_asr,\n",
    "    })\n",
    "\n",
    "    with open(\"results/mistral_attacks.json\", \"w\") as f:\n",
    "        json.dump(mistral_global_attacks, f, indent=2)\n",
    "\n",
    "    with open(\"results/llama_attacks.json\", \"w\") as f:\n",
    "        json.dump(llama_global_attacks, f, indent=2)\n",
    "\n",
    "    with open(\"results/mistral_history.json\", \"w\") as f:\n",
    "        json.dump(mistral_history, f, indent=2)\n",
    "\n",
    "    with open(\"results/llama_history.json\", \"w\") as f:\n",
    "        json.dump(llama_history, f, indent=2)\n",
    "\n",
    "    current_bases = list(set(mistral_successes + llama_successes))\n",
    "\n",
    "    if not current_bases:\n",
    "        print(\"No successful attacks this round – stopping early.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_total = len(mistral_global_attacks)\n",
    "m_success = sum(1 for r in mistral_global_attacks if r[\"unsafe_marked\"])\n",
    "\n",
    "l_total = len(llama_global_attacks)\n",
    "l_success = sum(1 for r in llama_global_attacks if r[\"unsafe_marked\"])\n",
    "\n",
    "print(\"\\n=== Overall summary ===\")\n",
    "print(f\"Mistral: {m_success} / {m_total} successful attacks\")\n",
    "print(f\"LLaMA:   {l_success} / {l_total} successful attacks\")\n",
    "\n",
    "m_asr_overall = m_success / max(1, m_total)\n",
    "l_asr_overall = l_success / max(1, l_total)\n",
    "\n",
    "m_asr_overall, l_asr_overall\n",
    "\n",
    "summary = {\n",
    "    \"m_total\": m_total,\n",
    "    \"m_success\": m_success,\n",
    "    \"m_asr_overall\": m_asr_overall,\n",
    "    \"l_total\": l_total,\n",
    "    \"l_success\": l_success,\n",
    "    \"l_asr_overall\": l_asr_overall,\n",
    "}\n",
    "\n",
    "with open(\"results/overall_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "models = [\"Mistral\", \"LLaMA\"]\n",
    "success_counts = [m_success, l_success]\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.bar(models, success_counts)\n",
    "plt.title(\"Number of Successful Attacks\")\n",
    "plt.ylabel(\"# Successful Attacks\")\n",
    "plt.savefig(\"plots/success_counts.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_values = [m_asr_overall, l_asr_overall]\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.bar(models, asr_values)\n",
    "plt.title(\"Attack Success Rate (ASR)\")\n",
    "plt.ylabel(\"ASR\")\n",
    "plt.ylim(0, 1)\n",
    "plt.savefig(\"plots/asr_overall.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mistral = pd.DataFrame(mistral_history)\n",
    "df_llama = pd.DataFrame(llama_history)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(df_mistral[\"round\"], df_mistral[\"ASR\"], marker=\"o\")\n",
    "plt.title(\"Mistral – ASR Over Rounds\")\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"ASR\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"plots/mistral_asr_over_rounds.png\", dpi=200, bbox_inches=\"tight\") \n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(df_llama[\"round\"], df_llama[\"ASR\"], marker=\"s\")\n",
    "plt.title(\"LLaMA – ASR Over Rounds\")\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"ASR\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"plots/llama_asr_over_rounds.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(df_mistral[\"round\"], df_mistral[\"ASR\"], marker=\"o\", label=\"Mistral\")\n",
    "plt.plot(df_llama[\"round\"], df_llama[\"ASR\"], marker=\"s\", label=\"LLaMA\")\n",
    "plt.title(\"ASR Comparison: Mistral vs LLaMA Over Rounds\")\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"ASR\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(\"plots/asr_comparison_mistral_vs_llama.png\", dpi=200, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
