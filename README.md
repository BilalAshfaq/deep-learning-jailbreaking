# Deep Learning Project 
## Evaluation of Jailbreaking in Open-Source Language Models

In this project we have shown how the evolution of prompts can impact jailbreaking behaviour in the open source models, for this research purpose we have picked two well known models "mistralai/Mistral-7B-Instruct-v0.3" by Mistral AI and "meta-llama/Llama-3.1-8B" by Meta.


